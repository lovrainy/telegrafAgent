// +build !solaris

package logparser_es

import (
	"bytes"
	"crypto/md5"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"os"
	"path"
	"path/filepath"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"syscall"
	"time"

	"github.com/influxdata/tail"

	"github.com/influxdata/telegraf"
	"github.com/influxdata/telegraf/internal/globpath"
	"github.com/influxdata/telegraf/plugins/inputs"
	"github.com/influxdata/telegraf/plugins/parsers"
)

const (
	defaultWatchMethod = "inotify"
)

type Tail struct {
	Files         []string
	FromBeginning bool
	Pipe          bool
	WatchMethod   string
	Watchdir string
	Tail_interval float64

	tailers    map[string]*tail.Tail
	parserFunc parsers.ParserFunc
	wg         sync.WaitGroup
	acc        telegraf.Accumulator

	sync.Mutex

	MultilineConfig MultilineConfig `toml:"multiline"`
	multiline       *Multiline
}

type FileWatch struct {
	name string  // log文件名称
	watchfile string // 缓存文件名称
	inode uint64 // log文件的inode
	offset int64  // seek位移
	watchmd5 string // n行内容的md5记录
}
var filewatchMap = make(map[string]*FileWatch)

func NewTail() *Tail {
	return &Tail{
		FromBeginning: false,
	}
}

const sampleConfig = `
  ## files to tail.
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". ie:
  ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
  ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
  ##   "/var/log/apache.log" -> just tail the apache log file
  ##
  ## See https://github.com/gobwas/glob for more examples
  ##
  files = ["/var/mymetrics.out"]
  ## Read file from beginning.
  from_beginning = false
  ## Whether file is a named pipe
  pipe = false

  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  # watch_method = "inotify"

  ## Data format to consume.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"

  ## multiline parser/codec
  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
  #[inputs.logparser.multiline]
    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
    #pattern = "^\s"

    ## The what must be previous or next and indicates the relation to the multi-line event.
    #what = "previous"

    ## The negate can be true or false (defaults to false). 
    ## If true, a message not matching the pattern will constitute a match of the multiline filter and the what will be applied. (vice-versa is also true)
    #negate = false

    #After the specified timeout, this plugin sends the multiline event even if no new pattern is found to start a new event. The default is 5s.
		#timeout = 5s
`

func (t *Tail) SampleConfig() string {
	return sampleConfig
}

func (t *Tail) Description() string {
	return "Stream a log file, like the tail -f command"
}

func (t *Tail) Gather(acc telegraf.Accumulator) error {
	t.Lock()
	defer t.Unlock()

	return t.tailNewFiles(true)
}

// 流程： Start ---> tailNewfiles ---> receiver(此处做行数与md5记录)
// 20191006: 生成md5方法
func md5Str(str string) string {
	w := md5.New()
	io.WriteString(w, str)
	md5str := fmt.Sprintf("%x", w.Sum(nil))
	return md5str
}

// 20191009更新：文件正则查找（目前支持单层(*.)后缀匹配，嵌套(**.后缀)）
func findfile(root string) []string {
	rootDir := filepath.Dir(root)
	exprOld := filepath.Base(root)

	var expr string

	var fileList []string
	if exprOld[:1] == "*" || exprOld[:2] == "**"{
		if exprOld[:1] == "*" {
			expr = ".*" + "\\" + strings.Split(exprOld, "*")[1]
		}
		if exprOld[:2] == "**" {
			expr = ".*" + "\\" + strings.Split(exprOld, "**")[1]
		}
		regex, err := regexp.Compile(expr)
		if err != nil {
			fmt.Println("Invalid regular expression.")
			os.Exit(1)
		}
		_ = filepath.Walk(rootDir, func(path string, info os.FileInfo, err error) error {
			if err != nil {
			}
			if regex.MatchString(info.Name()) {
				//fmt.Println(filepath.Dir(path))
				if filepath.Dir(path) == rootDir &&  exprOld[:1] == "*" {
					fileList = append(fileList, path)
				} else if filepath.Dir(path) == rootDir &&  exprOld[:2] == "**" {
					fileList = append(fileList, path)
				}
			}
			return nil
		})
	} else {
		fileList = append(fileList, root)
	}
	return fileList
}

func (t *Tail) Start(acc telegraf.Accumulator) error {
	t.Lock()
	defer t.Unlock()
	// 第一步：判断缓存文件是否存在, 不存在即创建
	watchdir := t.Watchdir
	_, watcherr := os.Stat(watchdir)
	if os.IsNotExist(watcherr) {
		watcherr = os.Mkdir(watchdir, os.FileMode(0755))
		if watcherr != nil {
			// TODO: 正式版移除panic
			panic(watcherr)
		}
	}
	// 修正正则匹配的文件列表
	var fixFileList []string
	for _, filename := range t.Files {
		for _, newFile := range findfile(filename) {
			fixFileList = append(fixFileList, newFile)
		}
	}
	t.Files = fixFileList

	for _, filename := range t.Files {
		filewatch := FileWatch{name: filename}
		// 初始化当前文件缓存记录文件地址
		filewatch.watchfile = path.Join(watchdir,  path.Base(filename))

		// 文件不存在，则panic退出
		if _, err := os.Stat(filename); os.IsNotExist(err) {
			panic("filename: " + filename + " does not exist!")
		} else {
			// 获取当前文件inode
			var filestat syscall.Stat_t
			if err := syscall.Stat(filename, &filestat); err != nil {
				panic(err)
			}
			filewatch.inode = filestat.Ino
			// 初始化md5
			filewatch.watchmd5 = md5Str(filename + strconv.FormatUint(filewatch.inode, 10))
		}

		_, err := os.Stat(filewatch.watchfile)
		if os.IsNotExist(err) {
			// 缓存文件不存在，则创建
			f, err := os.Create(filewatch.watchfile)
			f.Close()
			if err != nil {
				panic(err)
			} else {
				// 初始化offset
				filewatch.offset = 0
			}
		} else {
			// 缓存文件存在则读取信息
			b, err := ioutil.ReadFile(filewatch.watchfile)
			if err != nil {
				panic(err)
			} else {
				bString := string(b)
				// 缓存文件非空才做读取处理
				if bString != "" {
					// 获取上次记录的offset信息
					bSplit := strings.Split(bString, "\n")
					filewatch.inode, _ = strconv.ParseUint(bSplit[0], 10, 64)
					filewatch.offset, _ = strconv.ParseInt(bSplit[1], 10, 64)
					// 获取上次记录的md5
					oldwatchmd5 := bSplit[2]
					// 对上次记录的md5与本次的md5做对比，不同则清空文件内容,offset初始化为0
					if oldwatchmd5 != filewatch.watchmd5 {
						filewatch.offset = 0
						f, _ := os.OpenFile(filewatch.name, os.O_WRONLY| os.O_TRUNC, 0600)
						f.Close()
					}
				}
			}
		}
		// 至此，首次打开：已记录filename、inode、watchfile、行数归零
		// 重新打开：已记录filename、上次记录的inode、watchfile、上次记录的行数、上次记录的n行内容的md5
		filewatchMap[filename] = &filewatch
	}

	var err error
	t.multiline, err = t.MultilineConfig.NewMultiline()
	if err != nil {
		return err
	}

	t.acc = acc
	t.tailers = make(map[string]*tail.Tail)

	return t.tailNewFiles(t.FromBeginning)
}

func (t *Tail) tailNewFiles(fromBeginning bool) error {
	var seek *tail.SeekInfo
	if !t.Pipe && !fromBeginning {
		seek = &tail.SeekInfo{
			Whence: 2,
			Offset: 0,
		}
	}

	var poll bool
	if t.WatchMethod == "poll" {
		poll = true
	}

	// Create a "tailer" for each file
	for _, filepath := range t.Files {
		g, err := globpath.Compile(filepath)
		if err != nil {
			t.acc.AddError(fmt.Errorf("E! Error Glob %s failed to compile, %s", filepath, err))
		}
		for _, file := range g.Match() {
			if _, ok := t.tailers[file]; ok {
				// we're already tailing this file
				continue
			}

			tailer, err := tail.TailFile(file,
				tail.Config{
					ReOpen:    true,
					Follow:    true,
					Location:  seek,
					MustExist: true,
					Poll:      poll,
					Pipe:      t.Pipe,
					Logger:    tail.DiscardingLogger,
				})
			if err != nil {
				t.acc.AddError(err)
				continue
			}

			log.Printf("D! [inputs.tail] tail added for file: %v", file)

			parser, err := t.parserFunc()
			if err != nil {
				t.acc.AddError(fmt.Errorf("error creating parser: %v", err))
			}

			// create a goroutine for each "tailer"
			t.wg.Add(1)
			if t.multiline.IsEnabled() {
				go t.receiverMultiline(parser, tailer)
			} else {
				go t.receiver(parser, tailer)
			}
			t.tailers[tailer.Filename] = tailer
		}
	}
	return nil
}

// this is launched as a goroutine to continuously watch a tailed logfile
// for changes, parse any incoming msgs, and add to the accumulator.
func (t *Tail) receiver(parser parsers.Parser, tailer *tail.Tail) {
	defer t.wg.Done()

	var firstLine = true
	var line *tail.Line

	for line = range tailer.Lines {
		// 20191006: 每次都重新记录log的offset
		// fmt.Println(line)
		offset, _ := tailer.Tell()
		filewatchMap[tailer.Filename].offset = offset
		// 每次都重新校验md5，且记录到缓存文件。
		var filestat syscall.Stat_t
		if err := syscall.Stat(tailer.Filename, &filestat); err != nil {
			panic(err)
		}
		inode := filestat.Ino
		filewatchMap[tailer.Filename].inode = filestat.Ino
		filewatchMap[tailer.Filename].watchmd5 = md5Str(tailer.Filename + strconv.FormatUint(inode, 10))
		// 重新记录到文件
		f, err1 := os.OpenFile(filewatchMap[tailer.Filename].watchfile, os.O_WRONLY | os.O_TRUNC, 0644)
		if err1 != nil {
			panic(err1)
		}
		content := strconv.FormatUint(inode, 10) + "\n" + strconv.FormatInt(offset, 10) + "\n" + filewatchMap[tailer.Filename].watchmd5
		_, err := f.WriteString(content)
		if err != nil {
			panic(err)
		}
		f.Close()

		if text := t.getLineText(line, tailer.Filename); text == "" {
			continue
		} else {
			t.parse(parser, tailer.Filename, text, &firstLine)
		}
	}
	t.handleTailerExit(tailer)
}

// same as the receiver method but multiline aware
// it buffers lines according to the multiline class
// it uses timeout channel to flush buffered lines
func (t *Tail) receiverMultiline(parser parsers.Parser, tailer *tail.Tail) {
	defer t.wg.Done()

	var buffer bytes.Buffer
	var firstLine = true
	for {
		var line *tail.Line
		timer := time.NewTimer(t.MultilineConfig.Timeout.Duration)
		timeout := timer.C
		var channelOpen bool

		// 20191006: 每次都重新记录log的offset
		// fmt.Println(line)
		offset, _ := tailer.Tell()
		filewatchMap[tailer.Filename].offset = offset
		// 每次都重新校验md5，且记录到缓存文件。
		var filestat syscall.Stat_t
		if err := syscall.Stat(tailer.Filename, &filestat); err != nil {
			panic(err)
		}
		inode := filestat.Ino
		filewatchMap[tailer.Filename].inode = filestat.Ino
		filewatchMap[tailer.Filename].watchmd5 = md5Str(tailer.Filename + strconv.FormatUint(inode, 10))
		// 重新记录到文件
		f, err1 := os.OpenFile(filewatchMap[tailer.Filename].watchfile, os.O_WRONLY | os.O_TRUNC, 0644)
		if err1 != nil {
			panic(err1)
		}
		content := strconv.FormatUint(inode, 10) + "\n" + strconv.FormatInt(offset, 10) + "\n" + filewatchMap[tailer.Filename].watchmd5
		_, err := f.WriteString(content)
		if err != nil {
			panic(err)
		}
		f.Close()

		select {
		case line, channelOpen = <-tailer.Lines:
		case <-timeout:
		}

		timer.Stop()

		var text string
		if line != nil {
			if text = t.getLineText(line, tailer.Filename); text == "" {
				continue
			}
			if text = t.multiline.ProcessLine(text, &buffer); text == "" {
				continue
			}
		} else {
			//timeout or channel closed
			//flush buffer
			if text = t.multiline.Flush(&buffer); text == "" {
				continue
			}
		}
		t.parse(parser, tailer.Filename, text, &firstLine)

		if !channelOpen {
			break
		}
	}

	t.handleTailerExit(tailer)
}

func (t *Tail) handleTailerExit(tailer *tail.Tail) {
	log.Printf("D! [inputs.tail] tail removed for file: %v", tailer.Filename)

	if err := tailer.Err(); err != nil {
		t.acc.AddError(fmt.Errorf("E! Error tailing file %s, Error: %s\n",
			tailer.Filename, err))
	}
}

func (t *Tail) getLineText(line *tail.Line, filename string) string {
	if line.Err != nil {
		t.acc.AddError(fmt.Errorf("E! Error tailing file %s, Error: %s\n",
			filename, line.Err))
		return ""
	}
	// Fix up files with Windows line endings.
	return strings.TrimRight(line.Text, "\r")
}

func (t *Tail) parse(parser parsers.Parser, filename string, text string, firstLine *bool) {
	var err error
	var metrics []telegraf.Metric
	var m telegraf.Metric
	if *firstLine {
		metrics, err = parser.Parse([]byte(text))
		*firstLine = false
		if err == nil {
			if len(metrics) == 0 {
				return
			}
			m = metrics[0]
		}
	} else {
		m, err = parser.ParseLine(text)
	}

	if err == nil {
		if m != nil {
			tags := m.Tags()
			tags["path"] = filename
			t.acc.AddFields(m.Name(), m.Fields(), tags, m.Time())
		}
	} else {
		t.acc.AddError(fmt.Errorf("E! Malformed log line in %s: [%s], Error: %s\n",
			filename, text, err))
	}
}

func (t *Tail) Stop() {
	t.Lock()
	defer t.Unlock()

	for _, tailer := range t.tailers {
		err := tailer.Stop()
		if err != nil {
			t.acc.AddError(fmt.Errorf("E! Error stopping tail on file %s\n", tailer.Filename))
		}
	}

	for _, tailer := range t.tailers {
		tailer.Cleanup()
	}
	t.wg.Wait()
}

func (t *Tail) SetParserFunc(fn parsers.ParserFunc) {
	t.parserFunc = fn
}

func init() {
	inputs.Add("logparser_es", func() telegraf.Input {
		return NewTail()
	})
}
